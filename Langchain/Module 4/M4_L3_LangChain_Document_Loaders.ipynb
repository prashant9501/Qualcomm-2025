{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjyiD8I5r7jB"
      },
      "source": [
        "# Exploring Document Loaders in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSu-YKYjNs0F",
        "outputId": "82eb0d79-57e8-4c5e-d8d7-e1501c458119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.11 in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.11) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.11) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.11) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.11) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.11) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.11) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.2.2)\n",
            "Collecting langchain-openai==0.2.12\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.24)\n",
            "Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n",
            "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.2.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n",
            "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed langchain-openai-0.2.12 openai-1.57.4 tiktoken-0.8.0\n",
            "Collecting langchain-community==0.3.11\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.11 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.3.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.3.24)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.11 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.11\n",
        "!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CB6lHzbz5a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4356e612-f06c-4108-c6f8-2bc6774fa65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured==0.14.0 (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading unstructured-0.14.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.12.3)\n",
            "Collecting emoji (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Collecting pikepdf (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]==0.14.0) (3.4.2)\n",
            "Collecting pdf2image (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pypandoc (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting onnx (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting python-docx (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pypdf (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]==0.14.0) (2.2.2)\n",
            "Collecting pillow-heif (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pillow_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]==0.14.0) (3.7)\n",
            "Collecting msg-parser (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting unstructured-inference==0.7.31 (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading unstructured_inference-0.7.31-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]==0.14.0) (3.1.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]==0.14.0) (2.0.1)\n",
            "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.26.5)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.10.0.84)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.46.3)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0) (11.0.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]==0.14.0) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (4.25.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured[all-docs]==0.14.0)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.66.6)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]==0.14.0) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]==0.14.0) (1.2.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2024.8.30)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.28.1)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.6.0)\n",
            "Collecting pydantic<2.10.0,>=2.9.2 (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.14.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0)\n",
            "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.4.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2024.10.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.13.1)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.20.1+cu121)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.0.12)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.3.0)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs]==0.14.0)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.2.0)\n",
            "Downloading unstructured-0.14.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.7.31-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=3d2c6d35eee4e35ba45fef289137c3130ae3dc8aaccfa8725a1ec176ca33c760\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=c0f300b1af10e03b1410e724fab2a6163dbc84d52a8880444b574ba5375c8754\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=986f27382f11c2efa94558c811664b09c1b0532265d13c0a2a1175358f338a80\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pytesseract, pypdfium2, pypdf, pypandoc, pydantic-core, portalocker, pillow-heif, pdf2image, onnx, omegaconf, olefile, langdetect, jsonpath-python, humanfriendly, emoji, backoff, aiofiles, python-pptx, pydantic, pikepdf, msg-parser, iopath, coloredlogs, unstructured-client, pdfminer.six, onnxruntime, unstructured, pdfplumber, layoutparser, google-cloud-vision, effdet, unstructured-inference\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.1\n",
            "    Uninstalling pydantic_core-2.27.1:\n",
            "      Successfully uninstalled pydantic_core-2.27.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.3\n",
            "    Uninstalling pydantic-2.10.3:\n",
            "      Successfully uninstalled pydantic-2.10.3\n",
            "Successfully installed XlsxWriter-3.2.0 aiofiles-24.1.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 effdet-0.4.1 emoji-2.14.0 filetype-1.2.0 google-cloud-vision-3.9.0 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 msg-parser-1.2.0 olefile-0.47 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.20.1 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.4.2 pillow-heif-0.21.0 portalocker-3.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pypandoc-1.14 pypdf-5.1.0 pypdfium2-4.30.0 pytesseract-0.3.13 python-docx-1.1.2 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.19 python-pptx-0.6.23 rapidfuzz-3.10.1 unstructured-0.14.0 unstructured-client-0.28.1 unstructured-inference-0.7.31 unstructured.pytesseract-0.3.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "7f55e72d4bb84315aac9665eb41bb681"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# takes 2 - 5 mins to install on Colab\n",
        "!pip install \"unstructured[all-docs]==0.14.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTFImul36TRH"
      },
      "source": [
        "After installing `unstructured`above remember to restart your session when it shows you the following popup, if it doesn't go to `Runtime`and `Restart Session`\n",
        "\n",
        "![](https://i.imgur.com/UOBaotk.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NhEW-tOywUgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1eb626-c22d-4358-f41c-ea6e0481d141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,798 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123663 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# install OCR dependencies for unstructured\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MWNjOhSbRaOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371fee50-4e28-4984-92d8-bf448f872636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jq==1.7.0\n",
            "  Downloading jq-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Downloading jq-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (657 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/657.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m655.4/657.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.6/657.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.7.0\n",
            "Collecting pypdf==4.2.0\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf==4.2.0) (4.12.2)\n",
            "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "  Attempting uninstall: pypdf\n",
            "    Found existing installation: pypdf 5.1.0\n",
            "    Uninstalling pypdf-5.1.0:\n",
            "      Successfully uninstalled pypdf-5.1.0\n",
            "Successfully installed pypdf-4.2.0\n",
            "Collecting pymupdf==1.24.4\n",
            "  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.3 (from pymupdf==1.24.4)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.4\n"
          ]
        }
      ],
      "source": [
        "!pip install jq==1.7.0\n",
        "!pip install pypdf==4.2.0\n",
        "!pip install pymupdf==1.24.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqX0BkkWZ_e0"
      },
      "source": [
        "## Document Loaders\n",
        "\n",
        "Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n",
        "\n",
        "### Examples of Document Loaders:\n",
        "\n",
        "- **Text File Loader:** Loads data from a simple `.txt` file.\n",
        "- **Web Page Loader:** Retrieves the text content from any web page.\n",
        "- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n",
        "\n",
        "### Functionality:\n",
        "\n",
        "- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n",
        "- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
        "\n",
        "For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEx_nNkHLqZY"
      },
      "source": [
        "### Text Loader\n",
        "\n",
        "The simplest loader reads in a file as text and places it all into one document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Al7y4r93LKA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9205e51a-6f0e-490c-e1f8-34250e7a4d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  9834  100  9834    0     0  39173      0 --:--:-- --:--:-- --:--:-- 39336\n"
          ]
        }
      ],
      "source": [
        "!curl -o README.md https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ehpI19eLKEo"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./README.md\")\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SxR60glzvzQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6152598-37ca-4d31-cc13-ad18a130256b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uu-7i1PcdHpE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f1ba3f19-e093-411a-aa22-e96fb04f12a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 262);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FFbJx7Q9LKG4",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9165d3-d585-4f9b-e3b2-e592f20c8898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 🦜️🔗 LangChain\n",
            "\n",
            "⚡ Build context-aware reasoning applications ⚡\n",
            "\n",
            "[![Release Notes](https://img.shiel\n"
          ]
        }
      ],
      "source": [
        "print(doc[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTf3za4x7CtK"
      },
      "source": [
        "### Markdown Loader\n",
        "\n",
        "Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n",
        "\n",
        "This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEmzUQK9_640"
      },
      "source": [
        "Load the whole document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download nltk packages if needed"
      ],
      "metadata": {
        "id": "YJt1cdnFOeP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD-IBE4POUwu",
        "outputId": "5426d839-b8c2-4266-de76-9a3d1c1b2699"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D1uZEc-f8TyV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode='single')\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DPkNbhXv7Z3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17ac1fd-e26e-4e3d-d46e-75917d897aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m547b2uc8vy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9422bb91-e0e7-4546-a1b7-873a7c1d559c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 262);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "type(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_gia_n-T8Ytn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dfafd1-ba2e-4092-b050-6a8854e6b27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦜️🔗 LangChain\n",
            "\n",
            "⚡ Build context-aware reasoning applications ⚡\n",
            "\n",
            "Looking for the JS/TS library? Check \n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVyZosgZ_-U8"
      },
      "source": [
        "Load document and separate based on elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SOPDX8w85MeG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode=\"elements\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dr4WVaEg-qTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172359b0-0eaa-4c53-fe4a-80eea4670b23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HpfQwXmD-rji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27ece4d-5a9d-4e96-b1c2-e85bbf4a3e8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a'}, page_content='🦜️🔗 LangChain'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '80d06543c0c2b75ca147f3509e518a47'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'd68276ff4183b272b9dec78754e769b1'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e26407512e7a4e24a519ceb1a9dc980d'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '738df4293c5c58dbaa314f6e31f2c15c'}, page_content='Quick Install'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '17c3a345872b736bc4edb6cdae73f45a'}, page_content='With pip:'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '0dbefd69ddc0b2c597762e19b29f8e27'}, page_content='bash\\npip install langchain'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '3c98bc06e5ef8caa9446bb6f1d33dde0'}, page_content='With conda:'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e171db3a99f130885e84934a210c7dd8'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': 'a8858c1e5d7fad28685e95bd1bbeacc1'}, page_content='🤔 What is LangChain?')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "docs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EUcCM72S-vHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f37dbe2-825f-4a7e-92ce-e8e9cf38a862"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Title': 20, 'NarrativeText': 17, 'ListItem': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter([doc.metadata['category'] for doc in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8Hb5sKABXD"
      },
      "source": [
        "Comparing Unstructured.io loaders vs LangChain wrapper API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RZqKGM8q8RpS"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.md import partition_md\n",
        "\n",
        "docs = partition_md(filename=\"./README.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eQES4WJY80IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5841ac6-7970-4682-916e-71289182bab9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "88F0QIA2-_96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9878bd-c9c0-4ec6-a2be-aa35fde42975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Title at 0x7f35fb040430>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35fb042110>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35fb040460>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35fb041c00>,\n",
              " <unstructured.documents.elements.Title at 0x7f35fb040b80>,\n",
              " <unstructured.documents.elements.Title at 0x7f35fb0409a0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35fb041720>,\n",
              " <unstructured.documents.elements.Title at 0x7f35fb040610>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35fb041e40>,\n",
              " <unstructured.documents.elements.Title at 0x7f35fb0421d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "docs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oVGTYLJf7fgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe389b6-fbdf-4ef2-ebfa-e6cf9afbbc49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'Title',\n",
              " 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
              " 'text': '🦜️🔗 LangChain',\n",
              " 'metadata': {'last_modified': '2024-12-13T18:12:29',\n",
              "  'languages': ['eng'],\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "docs[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FDMICcgV_GiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a3115f-6bc1-4a72-94d2-91fd66531434"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'NarrativeText',\n",
              " 'element_id': '80d06543c0c2b75ca147f3509e518a47',\n",
              " 'text': '⚡ Build context-aware reasoning applications ⚡',\n",
              " 'metadata': {'last_modified': '2024-12-13T18:12:29',\n",
              "  'languages': ['eng'],\n",
              "  'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "docs[1].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "g87ZcUNG_Ka_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a69dd92-925b-4db9-dec8-0905d5793c72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='🦜️🔗 LangChain'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Quick Install'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='With pip:'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='bash\\npip install langchain'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='With conda:'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
              " Document(metadata={'last_modified': '2024-12-13T18:12:29', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='🤔 What is LangChain?')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = [Document(page_content=doc.text,\n",
        "                    metadata=doc.metadata.to_dict())\n",
        "              for doc in docs]\n",
        "lc_docs[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSSBO_Y3fB8P"
      },
      "source": [
        "### CSV Loader\n",
        "\n",
        "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
        "\n",
        "LangChain implements a CSV Loader that will load CSV files into a sequence of `Document` objects. Each row of the CSV file is converted to one document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QuoCrfQUODLt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with some dummy real estate data\n",
        "data = {\n",
        "    'Property_ID': [101, 102, 103, 104, 105],\n",
        "    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n",
        "    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n",
        "    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n",
        "    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n",
        "    'Bedrooms': [3, 2, 4, 3, 5],\n",
        "    'Bathrooms': [2, 1, 3, 2, 4],\n",
        "    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hG-_cJFULKI3"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path=\"./data.csv\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Dgrc_HhzLKMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d644b1-2224-4e63-9fef-88a8f8d7041b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "R8vVzwmseaoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8349d972-89ab-4943-adb4-f4b0fc763619"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LdENGJXQOXA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a5cf0c-5ba9-4591-a831-dc0fbdd18869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Property_ID: 101\n",
            "Address: 123 Elm St\n",
            "City: Springfield\n",
            "State: CA\n",
            "Zip_Code: 98765\n",
            "Bedrooms: 3\n",
            "Bathrooms: 2\n",
            "Listing_Price: 500000\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HVFIEiiB1Wu"
      },
      "source": [
        "`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's csv.`DictReader`. See the [`csv` module](https://docs.python.org/3/library/csv.html) documentation for more information of what `csv` args are supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XxLiCAl2CHE2"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=\"./data.csv\",\n",
        "                   csv_args={\n",
        "                      \"delimiter\": \",\",\n",
        "                      \"quotechar\": '\"',\n",
        "                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n",
        "                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n",
        "                   },\n",
        "                  )\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8WuekvnIDShM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa3f04a-ebb1-4e77-87d0-f14a3907aabd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXseBM98BgkG"
      },
      "source": [
        "Unstructured.io loads the entire CSV as a single table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "89SAdDs7A4eR"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
        "\n",
        "loader = UnstructuredCSVLoader(\"./data.csv\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uvic6kW8BEn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add1135c-ade4-41c9-db89-bf49bca703c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "54gFzsugBnad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcbc766-affd-44cb-ec29-d8eb86020c33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv'}, page_content='\\n\\n\\nProperty_ID\\nAddress\\nCity\\nState\\nZip_Code\\nBedrooms\\nBathrooms\\nListing_Price\\n\\n\\n101\\n123 Elm St\\nSpringfield\\nCA\\n98765\\n3\\n2\\n500000\\n\\n\\n102\\n456 Oak St\\nRivertown\\nTX\\n87654\\n2\\n1\\n350000\\n\\n\\n103\\n789 Pine St\\nLaketown\\nFL\\n76543\\n4\\n3\\n600000\\n\\n\\n104\\n321 Maple St\\nHillside\\nNY\\n65432\\n3\\n2\\n475000\\n\\n\\n105\\n654 Cedar St\\nSunnyvale\\nCO\\n54321\\n5\\n4\\n750000\\n\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrHu4ts4fHgH"
      },
      "source": [
        "### JSON Loader\n",
        "\n",
        "[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
        "\n",
        "[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n",
        "\n",
        "LangChain implements a [JSONLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain `Document` objects. It uses a specified [`jq` schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
        "\n",
        "It uses the `jq` python package. Check out [this manual](https://jqlang.github.io/jq/manual/) for a detailed documentation of the `jq` syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ALFBPgsnOd0L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Sample data dictionary similar to the one you provided but with modified contents\n",
        "data = {\n",
        "    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n",
        "    'is_still_participant': True,\n",
        "    'joinable_mode': {'link': '', 'mode': 1},\n",
        "    'magic_words': [],\n",
        "    'messages': [\n",
        "        {'content': 'See you soon!',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675597571851},\n",
        "        {'content': 'Thanks for the update! See you then.',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675597435669},\n",
        "        {'content': 'Actually, the green one is sold out.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675596277579},\n",
        "        {'content': 'I was hoping to purchase the green one!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595140251},\n",
        "        {'content': 'I’m really interested in the green one, not the red!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595109305},\n",
        "        {'content': 'Here’s the $150 for it.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595068468},\n",
        "        {'photos': [{'creation_timestamp': 1675595059,\n",
        "                     'uri': 'image_of_the_item.jpg'}],\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595060730},\n",
        "        {'content': 'It typically sells for at least $200 online',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595045152},\n",
        "        {'content': 'How much are you asking?',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675594799696},\n",
        "        {'content': 'Good morning! $50 is far too low.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675577876645},\n",
        "        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675549022673}\n",
        "    ],\n",
        "    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n",
        "    'thread_path': 'inbox/User A and User B chat',\n",
        "    'title': 'User A and User B chat'\n",
        "}\n",
        "\n",
        "# Save the modified data to a JSON file\n",
        "with open('chat_data.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ9YwY4xG7KD"
      },
      "source": [
        "To load the full data as a single document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aD1kjnQRQ0h5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EQIAvS8rGpNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f05f60-5a26-435f-aa06-f8947a7625c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dPmHC3aYRgFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c858b29-c7ba-49b2-a998-9fba40162360"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY48CWHhG_KF"
      },
      "source": [
        "Suppose we are interested in extracting the values under the `messages` key of the JSON data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VC5cWidHSFuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76200c4a-b2ca-4652-bf8f-c79a08a1796c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[]',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK59yCK3H-C-"
      },
      "source": [
        "Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "n7Bq2FHlSVb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00159a03-2bfa-40b3-b522-923494450ee3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content=''),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZB2fxI9fKxC"
      },
      "source": [
        "### PDF Loaders\n",
        "\n",
        "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
        "\n",
        "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n",
        "\n",
        "Here we will see how to load PDF documents into the LangChain `Document` format\n",
        "\n",
        "We download a research paper to experiment with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2NWiC51KDbm"
      },
      "source": [
        "If the following command fails you can download the paper manually by going to http://arxiv.org/pdf/2103.15348.pdf, save it as `layoutparser_paper.pdf`and upload it on the left in Colab from the upload files option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "t_zMe1cES7Tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2f1bfe-3adf-4c57-835f-847011c9fd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 18:14:26--  http://arxiv.org/pdf/2103.15348.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2103.15348 [following]\n",
            "--2024-12-13 18:14:26--  http://arxiv.org/pdf/2103.15348\n",
            "Reusing existing connection to arxiv.org:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4686220 (4.5M) [application/pdf]\n",
            "Saving to: ‘layoutparser_paper.pdf’\n",
            "\n",
            "layoutparser_paper. 100%[===================>]   4.47M  9.96MB/s    in 0.4s    \n",
            "\n",
            "2024-12-13 18:14:27 (9.96 MB/s) - ‘layoutparser_paper.pdf’ saved [4686220/4686220]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs1ZBxbxfNXq"
      },
      "source": [
        "#### PyPDFLoader\n",
        "\n",
        "Here we load a PDF using `pypdf` into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "a0be_BLqTI0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f51f6d-28cc-4572-bd93-e518335d96a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
            "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gJZo2Hk9hMgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bb138e-e856-4e50-f1aa-f860dd939168"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XrqsL5lhToUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f08a24d-2b81-4531-a05f-02739c6ba008"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'page': 0}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8xU5qyqJT7TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52aa4dff-0efe-4fce-960c-de64dfefd100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3Harvard University\n",
            "{melissadell,jacob carlson }@fas.harvard.edu\n",
            "4University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser , an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io .\n",
            "Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n",
            "·Character Recognition ·Open Source library ·Toolkit.\n",
            "1 Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"
          ]
        }
      ],
      "source": [
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KcDMgTfBTrZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79b1ed5-c5df-479c-9ba3-9a2ab99f1208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n",
            "Table 1: Current layout detection models in the LayoutParser model zoo\n",
            "Dataset Base Model1Large Model Notes\n",
            "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
            "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
            "Newspaper [17] F - Layouts of scanned US newspapers from the 20th century\n",
            "TableBank [18] F F Table region on modern scientiﬁc and business document\n",
            "HJDataset [31] F / M - Layouts of history Japanese documents\n",
            "1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\n",
            "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
            "backbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\n",
            "R-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
            "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
            "zoo in coming months.\n",
            "layout data structures , which are optimized for eﬃciency and versatility. 3) When\n",
            "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
            "API provided in the OCR module . 4)LayoutParser comes with a set of utility\n",
            "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
            "is also highly customizable, via its integration with functions for layout data\n",
            "annotation and model training . We now provide detailed descriptions for each\n",
            "component.\n",
            "3.1 Layout Detection Models\n",
            "InLayoutParser , a layout model takes a document image as an input and\n",
            "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
            "from traditional methods, it relies on deep convolutional neural networks rather\n",
            "than manually curated rules to identify content regions. It is formulated as an\n",
            "object detection problem and state-of-the-art models like Faster R-CNN [ 28] and\n",
            "Mask R-CNN [ 12] are used. This yields prediction results of high accuracy and\n",
            "makes it possible to build a concise, generalized interface for layout detection.\n",
            "LayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\n",
            "perform layout detection with only four lines of code in Python:\n",
            "1import layoutparser as lp\n",
            "2image = cv2. imread (\" image_file \") # load images\n",
            "3model = lp. Detectron2LayoutModel (\n",
            "4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n",
            "5layout = model . detect ( image )\n",
            "LayoutParser provides a wealth of pre-trained model weights using various\n",
            "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
            "domain shift [ 7], the prediction performance can notably drop when models are ap-\n",
            "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
            "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
            "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
            "is used for initializing the model weights in LayoutParser , using both the dataset\n",
            "name and model name lp://<dataset-name>/<model-architecture-name> .\n"
          ]
        }
      ],
      "source": [
        "print(pages[4].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_T0_7KtfUJj"
      },
      "source": [
        "#### PyMuPDFLoader\n",
        "\n",
        "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the `pymupdf` library internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "l3KTMV_3XmfL"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JXUAxQnJhuHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48f2e8a-7585-43dd-89f8-ee89610a1bed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "CZM5poERdRpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f431dce0-4df6-4f05-f2cb-9817abdd5f0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'file_path': './layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Uhr0Y1C90TH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60569239-3c1e-4b55-fb78-6986cf2d488b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': './layoutparser_paper.pdf',\n",
              " 'file_path': './layoutparser_paper.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 16,\n",
              " 'format': 'PDF 1.5',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'creator': 'LaTeX with hyperref',\n",
              " 'producer': 'pdfTeX-1.40.21',\n",
              " 'creationDate': 'D:20210622012710Z',\n",
              " 'modDate': 'D:20210622012710Z',\n",
              " 'trapped': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "pages[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BdaA9hkHXmhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81143811-c0ea-4a48-c7ce-e0287b50f876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser: A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1 Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2 Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3 Harvard University\n",
            "{melissadell,jacob carlson}@fas.harvard.edu\n",
            "4 University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5 University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser, an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io.\n",
            "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
            "· Character Recognition · Open Source library · Toolkit.\n",
            "1\n",
            "Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
            "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eDxTzEe0Le6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b7e4fa-71bf-42ad-ce87-75bed10a5371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n",
            "5\n",
            "Table 1: Current layout detection models in the LayoutParser model zoo\n",
            "Dataset\n",
            "Base Model1 Large Model\n",
            "Notes\n",
            "PubLayNet [38]\n",
            "F / M\n",
            "M\n",
            "Layouts of modern scientiﬁc documents\n",
            "PRImA [3]\n",
            "M\n",
            "-\n",
            "Layouts of scanned modern magazines and scientiﬁc reports\n",
            "Newspaper [17]\n",
            "F\n",
            "-\n",
            "Layouts of scanned US newspapers from the 20th century\n",
            "TableBank [18]\n",
            "F\n",
            "F\n",
            "Table region on modern scientiﬁc and business document\n",
            "HJDataset [31]\n",
            "F / M\n",
            "-\n",
            "Layouts of history Japanese documents\n",
            "1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\n",
            "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
            "backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\n",
            "R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
            "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
            "zoo in coming months.\n",
            "layout data structures, which are optimized for eﬃciency and versatility. 3) When\n",
            "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
            "API provided in the OCR module. 4) LayoutParser comes with a set of utility\n",
            "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
            "is also highly customizable, via its integration with functions for layout data\n",
            "annotation and model training. We now provide detailed descriptions for each\n",
            "component.\n",
            "3.1\n",
            "Layout Detection Models\n",
            "In LayoutParser, a layout model takes a document image as an input and\n",
            "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
            "from traditional methods, it relies on deep convolutional neural networks rather\n",
            "than manually curated rules to identify content regions. It is formulated as an\n",
            "object detection problem and state-of-the-art models like Faster R-CNN [28] and\n",
            "Mask R-CNN [12] are used. This yields prediction results of high accuracy and\n",
            "makes it possible to build a concise, generalized interface for layout detection.\n",
            "LayoutParser, built upon Detectron2 [35], provides a minimal API that can\n",
            "perform layout detection with only four lines of code in Python:\n",
            "1 import\n",
            "layoutparser as lp\n",
            "2 image = cv2.imread(\"image_file\") # load\n",
            "images\n",
            "3 model = lp. Detectron2LayoutModel (\n",
            "4\n",
            "\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\n",
            "5 layout = model.detect(image)\n",
            "LayoutParser provides a wealth of pre-trained model weights using various\n",
            "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
            "domain shift [7], the prediction performance can notably drop when models are ap-\n",
            "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
            "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
            "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
            "is used for initializing the model weights in LayoutParser, using both the dataset\n",
            "name and model name lp://<dataset-name>/<model-architecture-name>.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pages[4].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zvyk9ACL8fx"
      },
      "source": [
        "#### UnstructuredPDFLoader\n",
        "\n",
        "[Unstructured.io](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [`UnstructuredPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv4PBKyfR2od"
      },
      "source": [
        "Load PDF as a single document - no complex parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Z8qz1HKWMRTf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nDWTGeMFMbI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ae130a-2190-4898-dfdf-0b2d988ddda0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UUmEA9qfMdIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2af13e-0618-4e42-a667-fb7384ada142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 0 2\n",
            "\n",
            "n u J\n",
            "\n",
            "1 2\n",
            "\n",
            "]\n",
            "\n",
            "V C . s c [\n",
            "\n",
            "2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n",
            "\n",
            "LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n",
            "\n",
            "Zejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain Lee4, Jacob Carlson3, and Weining Li5\n",
            "\n",
            "1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n",
            "\n",
            "Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify d\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeB_kHTjR6u5"
      },
      "source": [
        "Load PDF with complex parsing, table detection and chunking by sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "e606cf098a0c47388ab4a2f387b3beac",
            "bbdf731fc0294dd6a3c392012bc93896",
            "63fc017f50be4f1f8c807f30a7ce11eb",
            "4a47716561f74d8fa006b47f64a6837a",
            "c69e38f11339431cac851d749a68ed9a",
            "b8aab8494a2045bdbc09415b1418bd81",
            "b4c4b57aa1f94e958c0d09c7558b7817",
            "79141af480654819ae6c0981732355a2",
            "8b217d584ee945a9bfedf1a6f33bfb67",
            "e6b2ee0c072e457db0d8989b8ed1aaac",
            "b8bffaaa7c524cacb9cb2a5f67581e7f",
            "dee9511a788d41bab9d5fdd4b6e4f6a9",
            "ff2dfe08ffed46d2a633ac0b7b0eeec5",
            "0f6d1f3099c2428bbce1f41dd04cdc4e",
            "cc62a8b1ab8045caa46adac2bad7aef2",
            "1bb6db9f42c047b8a65dcf4b93d84bbf",
            "7194f73d47a0456a8d462102d8b24288",
            "822a272cf0724edb8e7fdccd5317e86e",
            "43b3d3c90225407e943fc9dcb39ad53e",
            "fb61550b3a774725a24e4e2a9ace15d5",
            "1c7f0cf2749146668bd8df52ffe11fad",
            "63a6b02ceb9340aba44b6891be465f0c",
            "677e724e773541898d7682c8b8d8ff61",
            "67f5f48c907f47f7a64bfdef3fb25c8b",
            "3865ca39e1bc4a50ab761d215de760d7",
            "432ad00a5bd84f1e9ed0cf8c881bbd74",
            "1a85a1b9c4494ccfb66d5c351fdc42b3",
            "43b9cf97a2044db3a65ba7a013da8d80",
            "c5aa729abde54a6692fec1aa39c4f49c",
            "73c52584484948f1a8bd01beb4969315",
            "46b6d128201f4f479c45b231ece02270",
            "34994b355d1841ec9019ec2a7c5dd408",
            "68f35650c8ea40caa773596bf12c1845"
          ]
        },
        "id": "jbH6ZJLcNs0s",
        "outputId": "4a883f84-3780-4638-97d4-bb080b5b2e78"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e606cf098a0c47388ab4a2f387b3beac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee9511a788d41bab9d5fdd4b6e4f6a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "677e724e773541898d7682c8b8d8ff61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# takes 3-4 mins on Colab\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf',\n",
        "                               strategy='hi_res',\n",
        "                               extract_images_in_pdf=False,\n",
        "                               infer_table_structure=True,\n",
        "                               chunking_strategy=\"by_title\",\n",
        "                               max_characters=4000, # max size of chunks\n",
        "                               new_after_n_chars=3800, # preferred size of chunks\n",
        "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
        "                               mode='elements')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "0WvtzXCgMots",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c12de1c-3322-4d7e-ee27-0e82ac01fc69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JM5fCpJNPPiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4dfe3e-79ed-4682-e219-99b92ae0fda9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'Table',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'Table',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "[doc.metadata['category'] for doc in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "9f7zcYUxMq0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de586815-702d-4a29-b348-0702e0fb568b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2023-01-23T09:15:33', 'page_number': 1, 'orig_elements': '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', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'CompositeElement', 'element_id': 'a9d856e62be9762f29e02db1df238cf5'}, page_content='1 2 0 2\\n\\nn u J 1 2 ] V C . s c [\\n\\n2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\\n\\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\\n\\nZejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\\n\\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\\n\\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\\n\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XdjS77Fh0oKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e88fae-b94c-41d5-9a4c-f22a3fdf0ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 0 2\n",
            "\n",
            "n u J 1 2 ] V C . s c [\n",
            "\n",
            "2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n",
            "\n",
            "LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n",
            "\n",
            "Zejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\n",
            "\n",
            "1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n",
            "\n",
            "Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\n",
            "\n",
            "Keywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "six4KwFGPX-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8a3a98-7e9f-4dd7-855c-9506f30627f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'last_modified': '2023-01-23T09:15:33', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVm1vmzAQ/isnvuxLk/ASmrSqKq2aoq1qqmrtPmUIObaToIFB2KjN2v73nU1CaYCSTiQyPvzc+R4/d7B4tnjMEy5UGDHrHCybceZRnw4cn7DB2B77A3JK6cD2XHfinC79lU+tE7ASrggjiiDm2aJpmrNIEMWlmcdkmxYq3PBovVFocV3bRszO/BgxtUGrMzHWLI2E0rjFwpucDf0T8Kbe0A5OYD/3J/bwVM8dz3WG4xZDiUCLJbdS8URnchc98fg+I5Rbr/iAccWpilIR0phIGWZ5usRl9vDMdqcOLlhFMVfbjBvs3dwyGxbrgqxNVguLi7UVGKtUYZKyaBVxw5lru97Adgau92CfnTv+uedpdIbIUBTJkue4CvOwFFnGPCQypDyOjVPNnVBIv/bzDfmUXGms4QfHx934pLeK41aPmM473F2xvCHbW67gahq89MGdBvznj+RrH8o9RN3yR5mRDHPrQXqHyAfNwhURf/qQ40Pk9+sjKfIPkS9whTiYp4zHXzopcroYno3mfZgGrb2IBqWzPkSDyl5Eg8IjMmkh7zbVpd2Bc7tYuzH1LiFdAZYLzwVIGuEzLBwKLKWF7ju9bhvE1txKSoTgbO8+IWvyNxJcAhGsHiznWZp3qsbtOpCWSL/uQeyVL2GVpwmoDQfXVhugCCvybV+Y9oLAPa6xOwH+mmTpdJaF1KnJirm+OI2zr6WziaRK8y1ck4ygU378cfivugcq/qR0I9uoJNauL0xru7xALgjTw+WuUi9GeK/n7yuwZjbaKuejPTzHP7t839fwKTNmlHB1f5zEyuUj43bnWve8ykurv89pqxmiapBvG/8ozBHCasaoWmlLjP9UVTNI1XX7TqBbVDWno1Ip5p2sVVR78dUU4sANyde7SSkReJPDwpsGYI4QbwOomMYPgkkAFSk4xXXV9nGtE8AMRjDHa2au8n6A1wz/nWr6XayWtlNTFHxSKHsHpVha4UcIAD4+032Q1nOFo6t//xFkglmvwT953xFI', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'Table', 'element_id': '720a11c7a3fa16628248e6b9613d2c2d'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "data[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AYe9e5K3Pfje",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9a804f43-5ec7-4166-c7e8-b727acf81959"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "data[5].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "nFI5BBDjPoHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "44047255-ab79-48bb-ba22-d9ad390e8598"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(data[5].metadata['text_as_html'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbzDKmPzSA1H"
      },
      "source": [
        "Load using raw unstructured.io APIs for PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_-wpOxHUMwOw"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements - takes 3-4 mins\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=\"./layoutparser_paper.pdf\",\n",
        "    strategy='hi_res',\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=\"./\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "u7QC4wt-St_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19e5870-d93b-4463-faa7-ccb8eda69504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "len(raw_pdf_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bC6seR7TSZY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a455dc2-a0e8-42cb-8af2-d85ed339facb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.CompositeElement at 0x7f34f14414e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f14436d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f35f4fd3580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f35f4f462c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0df57b0>,\n",
              " <unstructured.documents.elements.Table at 0x7f35ee6d7460>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f35ee6d4760>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f76230>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f775b0>,\n",
              " <unstructured.documents.elements.Table at 0x7f34f0f77460>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f76ad0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f77b80>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f77e50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f77df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f77e80>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f775e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f0f77d00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f34f125bfd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "raw_pdf_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8iPAwY5YS0CC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b132c46-51a6-46f2-c413-35d71cbb9875"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'Table',\n",
              " 'element_id': '720a11c7a3fa16628248e6b9613d2c2d',\n",
              " 'text': 'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents',\n",
              " 'metadata': {'last_modified': '2023-01-23T09:15:33',\n",
              "  'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\",\n",
              "  'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'},\n",
              "   {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'},\n",
              "   {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'},\n",
              "   {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'},\n",
              "   {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'},\n",
              "   {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'},\n",
              "   {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"},\n",
              "   {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
              "   {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'},\n",
              "   {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'},\n",
              "   {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'},\n",
              "   {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
              "   {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'},\n",
              "   {'x': 2,\n",
              "    'y': 1,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of modern scientific documents'},\n",
              "   {'x': 2,\n",
              "    'y': 2,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of scanned modern magazines and scientific report'},\n",
              "   {'x': 2,\n",
              "    'y': 3,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of scanned US newspapers from the 20th century'},\n",
              "   {'x': 2,\n",
              "    'y': 4,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Table region on modern scientific and business document'},\n",
              "   {'x': 2,\n",
              "    'y': 5,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of history Japanese documents'}],\n",
              "  'filetype': 'application/pdf',\n",
              "  'languages': ['eng'],\n",
              "  'page_number': 5,\n",
              "  'orig_elements': 'eJydVm1vmzAQ/isnvuxLk/CSNElVVVo1RVvVVNXafcoQMnBJ0MAgbNRmbf77zk5CaYCSTiQyPvzc+R4/d7B4MTDGBLn0otC4ACOcnJ+PkJm9qcmWvSFzhj2fmZMe2uNw7Dt+MA4c4wyMBCULmWSEeTGCNM3DiDOJQs9jtkkL6a0xWq0lWWzbNAmzNz9FoVyT1Rpra5ZGXCrcYuGMp/3RGTgTp2+6Z3CYj8Zm/1zNLce2+sMGww5BFkNshMREZXIfPWP8kLEAjS09CFFiIKOUe0HMhPCyPPVpmdmfmvbEogXLKEa5yVBj7+eG3jBfFWyls1oYyFeGq61CekkaRssINWe2aTs90+rZzqM5vbBGF46mKCOkx4vEx5xWUR6GZH6MHhNegHGsnSruuCT6lZ9vxKdAqbCaHxqf9uOz2iqNGzVSOu9w94V/yzZ3KOF64r52wa0a/OeP5GsXyj5G3eGTyFhGuXUgnWPko2LhmvE/XcjhMfL7zYkUjY6Rr3BNOJinIcZfWimy2hieDeZdmBqtnYgapbMuRI3KTkSNwhMyaSDvLlWl3YKz21i71fUuIF0ClQvmHEQQ0TMqnADCNChU3+l0WyO24lYEjHMMD+4TtmJ/I44CGA+rwXLM0rxVNXbbgTRE+vUA/KB8Acs8TUCuEWxTriEgWJFvusI0FwTtcUXdCehXJ0ul4xdCpSZK5rri1M6+ks46EjLNN3DDMkZO8fTjGG1VD5T4LFUjW8skVq4vdWu7uiQuWKiGq32lXg7oXs3fV2DFrLW1mw8O8Jz+4dX7vkZPQ20mCZf3p0lst3yg3e5dq55Xemn09zlt1UOUDfJt4x+FOUFY9RhlK22I8Z+qqgcpu27XCbSLquJ0sFOKficrFVVefBWFWHDL8tV+spMIvMlh4Uxc0EdIty6UTNMHwdiFkhSa0rpy+7TWcmEGA5jTNdPX7r5H14z+rWr6XSx906ooCj4plIODnVga4ScIAD4+00OQxnOFk6v/8BGkgxlb9x/yixGI',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'layoutparser_paper.pdf'}}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "raw_pdf_elements[5].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKjlNBQTHwE"
      },
      "source": [
        "Convert into LangChain `document`format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "iCFjyenoTMC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf91521e-1ec2-4020-9c09-ac7514ae8c28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'last_modified': '2023-01-23T09:15:33', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVm1P2zAQ/iunfNkX2uaFri1CSENTtSGKELBPXRQ58aWNljfFjqAD/vvObhtCk5AypZXji5873+PnLlk+Gxhjgqn0Im6cgTHmiOGM48AO/PHglE3CwTScWgNE9GczZk+CAI0TMBKUjDPJCPNsBFlW8ChlEoWex2yTldJbY7RaS7LYtmkSZmd+jLhck9WaaGueRalUuOXSmcyG4xNwps7QdE9gPx9PzOFXNbcc2xqethi2CLIYYiMkJiqT2+gJ4/uc0X5f6QFHiYGMstQLYiaElxeZT8vM4cy0pxYtCKMY5SZHjb1dGHrD6apkK53V0sB0ZbjaKqSXZDwKI9Sc2abtDExrYDsP5uzMGp85jkLnhPTSMvGxoFWUhyGZH6PHhBdgHGunirtUEv3Kz3fiU6BUWM0PjY+78UltlcaNGimdd7jb0r9mmxuUcDl1X/rgVgN+9zP51oeyD1E3+ChyllNuPUjnEPmgWLhk6Z8+5Okh8sfVkRSND5EvcEk4WGQc4y+dFFldDM9Hiz5Mg9ZeRIPSeR+iQWUvokHhEZm0kHeTqdLuwNldrF3reheQhUDlgkUKIojoGRVOADwLStV3et02iK25FQFLU+R79wlbsb9RigJYyuvBCsyzolM1dteBtET6dQ/pXvkCwiJLQK4RbFOuISBYWWz6wrQXBO1xRd0J6NckS6Xjl0KlJirm+uI0zr6WzjoSMis2cMVyRk7x+OMYv6oeKPFJqka2lkmsXJ/r1nZxTlwwroaLXaWej+hezd9XYM2stbWdj/bwgv784n1fo6dcm0nC1f1xEtsuH2m3O9eq51VeWv19TlvNEFWDfNv4R2GOEFYzRtVKW2L8p6qaQaqu23cC3aKqOR1tlaLfyUpFtRdfTSEWXLNitZtsJQJvclg6Uxf0EdKtCxXT9EEwcaEihaa0rto+rbVcmMMIFnTN9bW9H9A1p3+nmn6XoW9aNUXBJ4Wyd7AVSyv8CAHAx2e6D9J6rnB09e8/gnQw49X9B4S4EpY=', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = [Document(page_content=doc.text,\n",
        "                    metadata=doc.metadata.to_dict())\n",
        "              for doc in raw_pdf_elements]\n",
        "lc_docs[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQV0X4grW971"
      },
      "source": [
        "### Microsoft Office Document Loaders\n",
        "\n",
        "The Microsoft Office suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n",
        "\n",
        "[Unstructured.io](https://docs.unstructured.io/open-source/introduction/overview) provides a variety of document loaders to load MS Office documents. Check them out [here](https://docs.unstructured.io/open-source/core-functionality/partitioning).\n",
        "\n",
        "Here we will leverage LangChain's [`UnstructuredWordDocumentLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.word_document.UnstructuredWordDocumentLoader.html) to load data from a MS Word document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "V37elBlCT6Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479921a3-1c8a-4534-f578-13441241b863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-\n",
            "To: /content/Intel Strategy.docx\n",
            "\r  0% 0.00/1.13M [00:00<?, ?B/s]\r 92% 1.05M/1.13M [00:00<00:00, 10.4MB/s]\r100% 1.13M/1.13M [00:00<00:00, 11.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv2b1SZcX4zS"
      },
      "source": [
        "Load word doc as a single document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "BqYBSoqHT6TY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "\n",
        "loader = UnstructuredWordDocumentLoader('./Intel Strategy.docx')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "zfi8_V9MT6Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89376739-a47f-4591-b5a3-e2c4f85089bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "kznFjlj6T6Xw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "ac3bdf6e-c72a-4534-8023-d7c6f3fdef24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every indust'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "data[0].page_content[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILz-pXIyX8e0"
      },
      "source": [
        "Load word doc with complex parsing and section based chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7cKYWNjAUo1A"
      },
      "outputs": [],
      "source": [
        "loader = UnstructuredWordDocumentLoader('./Intel Strategy.docx',\n",
        "                                        strategy='fast',\n",
        "                                        chunking_strategy=\"by_title\",\n",
        "                                        max_characters=3000, # max limit of a document chunk\n",
        "                                        new_after_n_chars=2500, # preferred document chunk size\n",
        "                                        mode='elements')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "CkOn6t0QVOyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6531ab-db3f-455d-d3ab-fc667189655e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "IV4cp6yqVQZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126aae17-dc43-4625-d6c4-43a2b3424a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': '.', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2024-05-21T22:51:21', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}, page_content='Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\\n\\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\\n\\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "mW5UhN9BVTaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ca2b28-9e0b-4e6b-8de9-8ac5427f9668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Intel Strategy.docx', 'emphasized_text_contents': ['Intel IDM 2.0', 'Intel’s global, internal factory network for at-scale manufacturing', 'Expanded use of third-party foundry capacity.', 'Intel Foundry Services.', 'Expanding in the U.S. and Europe'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b'], 'file_directory': '.', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2024-05-21T22:51:21', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '9b9fec55d420d43d84e13646715885e6'}, page_content='Intel IDM 2.0\\n\\nThe Intel IDM 2.0 strategy revealed by new company CEO Pat Gelsinger\\xa0is an ambitious plan to restore the company’s leadership in semiconductor production. Gelsinger described IDM 2.0 as the second generation of Intel’s integrated device manufacturing model.\\n\\nIntel’s global, internal factory network for at-scale manufacturing\\xa0is a key competitive advantage that enables product optimization, improved economics and supply resilience. Gelsinger re-affirmed the company’s expectation to continue manufacturing most of its products internally. The company’s 7 Nanometer Processors development is driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow.\\n\\nExpanded use of third-party foundry capacity.\\xa0 Gelsinger said he expects Intel’s engagement with third-party foundries to grow and to include manufacturing for a range of modular tiles on advanced process technologies, including products at the core of Intel’s computing offerings for both client and data center segments beginning in 2023. This will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule and supply, giving the company a unique competitive advantage.\\n\\nIntel Foundry Services.\\xa0The launch of Intel Foundry Services means the company is not only going to manufacture its own chips, but it will also produce them for other semiconductor companies, including its competitors. \\xa0Intel announced plans to become a major provider of U.S. and Europe-based foundry capacity to serve the global demand for semiconductor manufacturing. Hence, Intel is establishing a new standalone business unit, Intel Foundry Services (IFS), led by semiconductor industry veteran Dr. Randhir Thakur, who will report directly to Gelsinger. IFS will be differentiated from other foundry offerings with a combination of leading-edge process technology and packaging, committed capacity in the U.S. and Europe, and a world-class IP portfolio for customers, including x86 cores as well as ARM and RISC-V ecosystem IPs. Gelsinger noted that Intel’s foundry plans have received strong statements of support from across the industry. Intel conservatively sizes the foundry opportunity as a $100 billion addressable market by 2025.\\n\\nExpanding in the U.S. and Europe. Intel is expanding its manufacturing capacity in the U.S. and Europe to provide less dependence on any specific region. Noting that 80% of leading-edge foundry capacity is concentrated in Asia, Gelsinger believes “the industry needs more geographically balanced manufacturing capacity.”')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "data[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "vVP9nzW_WwhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25416826-7929-4031-c1a8-98d0b4a29494"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Intel Strategy.docx', 'emphasized_text_contents': ['Intel Sustainability'], 'emphasized_text_tags': ['b'], 'file_directory': '.', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2024-05-21T22:51:21', 'orig_elements': 'eJzVVk1v3DYQ/SuDRT8ulqGvlVa+pU0QBDCKonZOcWBQ5EhLhCIVkvJ6G+S/91Fet0mRAu4pMHQRSM7M45vHR777tGHDE9t4q9XmgjZ5mVe8a/qsrmuV1ZJVJvJCZTtZd6Ks8rZVw+aMNhNHoUQUiPm0kSLy6PzxVvEc9xjKsYKneS+C/pPVbeT7eCudjagTMP1u8wb/hq6WEIW2otdGx+Pm/TeiohgfIvp1etCGb5X2LCPqJcDnm9OwFROngVPq6BOo47ly8j4tMcKOixj5IRvbcc1nRIi3k1N60Lzuv8zLOsu3WVlcl+XFtrgoi81nLExgvsj+NfA0f5zX6tc6Gk4R/yZW9qpuu2bImp0SWb1r86yTKFSonSr7Yde1Xfs0Yr87BzdLmRfyes+kp1nISG4gafSEYiT3qIGJQMLS4kcQQKNxvTAU955FPKffvYvAru1IbvE0AxVHUjwJqwJSTqx0SoXM2lnkUTR4Dnsk0PZDChO9WyLt3QFDTAfnjSI3c9ptOKcXgZzlBOrv2Z8DGRYqxQaeNKSolsQeqgY9PtRA+WVAzcWnZdJha1ZzOKMHMrEjjYW0WP1xYZpd0Cu86BD5AWgJBA7s2Uom60CKNccUkvboDvYEECFI2QP+mu0gjkAp4poDFSIx1M+eBoCTUJmb2CNgFj5a/K1IU8bD3hmmO2GWlXO9AokJyMTAbcdhMY8MRufO1p4pes0mYBIFgtDqnL6U7m/CJ4B3fJ36/C0Jd0pUW95mTZW8oRiarN9WXVZIhoYlvip/JhK+dgQxGvhMUhVonfrUziXQ6IR57DmcCHqJq44GZ4w7JGlozHk90QS0aJBF31K3YJz5xY29WfJc5P+L14L7fOjqPqsEw3iHrsuEqsusHbohr7Z9O2zlM+H1hdxrvmMq8vxHMGz5IHroFPuV0WsJs6QlpKPtXUhyD4/m8M/pOP+Su0sd4pvI07doa6u2FE23zVRXdlkN2rOdKpoMNstcF2rb9uUzoe2NvYOUSMyzd/erkcI7fqjyPInMpDOMAw42/XiEM9nA/k48mGNcSYRvpZsIdpVsQJyaUFN/ipbLtJhVg/RBQ8YiJvtc4CdwyVPeIO6g7qfTz03Hsm/7rC36Kqu3O7gBF3W2bcuyqSpVNu1zudB+WTRuEIg1EQkMicf12vmK1olhBW+v6LVntrTGJDv41S0Wy+jy1auXhP6NXkyE54FVwitYt7bSLOtKbBD0obPC2hTECpOp8YnV9XpJRvP26oxeLR4HYsXwImjx5J40jcQTIz3X6naLnsgy68tSZqIXtSwatWuKJ77evntPLgUY2uOKXK0i07ixQ/RH+uOnl2AKTXmQM/qiFQjQw5HG1BhcbXKf7nhhAh103BP0jsGTzxyEn1IvZpceoxojiWRkUTgxxs2rCkBXXGkl/rjoOf39Rwve/wWsUavu', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b8ae863a5df10800d4307a8d409a5034'}, page_content=\"Intel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said. \\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\xa0\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "data[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBJD6nbfYxg"
      },
      "source": [
        "### Directory Loaders\n",
        "\n",
        "LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Uvblyc6OZqpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69876af8-1402-4bee-a199-782a4404186a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 18:21:22--  https://arxiv.org/pdf/2010.11929.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2010.11929 [following]\n",
            "--2024-12-13 18:21:22--  http://arxiv.org/pdf/2010.11929\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3743814 (3.6M) [application/pdf]\n",
            "Saving to: ‘Vision Transformers.pdf’\n",
            "\n",
            "Vision Transformers 100%[===================>]   3.57M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-12-13 18:21:22 (66.3 MB/s) - ‘Vision Transformers.pdf’ saved [3743814/3743814]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O 'Vision Transformers.pdf' 'https://arxiv.org/pdf/2010.11929.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCXVqm3Ba2I8"
      },
      "source": [
        "We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n",
        "\n",
        "```\n",
        "loaders = {\n",
        "  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n",
        "}\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "- `file_format_extension` can be anything like `.docx`, `.pdf`etc.\n",
        "- `LoaderClass` is a specific data loader like `PyMuPDFLoader`\n",
        "- `LoaderKeywordArguments` are any specific keyword arguments which needs to be passed into that loader at runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "1zzlBioka1HR"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary to map file extensions to their respective loaders\n",
        "loaders = {\n",
        "    '.pdf': (PyMuPDFLoader, {}),\n",
        "    '.docx': (UnstructuredWordDocumentLoader, {'strategy': 'fast',\n",
        "                                              'chunking_strategy' : 'by_title',\n",
        "                                              'max_characters' : 3000, # max limit of a document chunk\n",
        "                                              'new_after_n_chars' : 2500, # preferred document chunk size\n",
        "                                              'mode' : 'elements'\n",
        "                                              })\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJZJtsGbnr7"
      },
      "source": [
        "`DirectoryLoader` accepts a `loader_cls` argument, which defaults to `UnstructuredLoader` but we can pass our own loaders which we defined above in the `loader_cls`argument and any keyword args for the loader can be passed in the `loader_kwargs` argument.\n",
        "\n",
        "We can also show a progress bar by setting `show_progress=True`\n",
        "\n",
        "We can use the `glob` parameter to control which files to load based on file patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mOmj9K-cB14"
      },
      "source": [
        "Here we create two separate loaders to load files which are word documents and PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "jbnt25HdXmkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17c2aef-1db8-48a1-e00b-47eadfdcfb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 12.06it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 10.34it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Define a function to create a DirectoryLoader for a specific file type\n",
        "def create_directory_loader(file_type, directory_path):\n",
        "    return DirectoryLoader(\n",
        "        path=directory_path,\n",
        "        glob=f\"**/*{file_type}\",\n",
        "        loader_cls=loaders[file_type][0],\n",
        "        loader_kwargs=loaders[file_type][1],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "# Create DirectoryLoader instances for each file type\n",
        "pdf_loader = create_directory_loader('.pdf', './')\n",
        "docx_loader = create_directory_loader('.docx', './')\n",
        "\n",
        "# Load the files\n",
        "pdf_documents = pdf_loader.load()\n",
        "docx_documents = docx_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "GXYc4p_eXmmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5aeae6-a64b-4104-93e6-dce2374ba599"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "len(pdf_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ze6DA3dGaTln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d9dd73-8eb2-4afd-8431-8002ed3f239c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'Vision Transformers.pdf', 'file_path': 'Vision Transformers.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "pdf_documents[18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "GRVEmiTvXmpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0ac7a9-48f3-4c02-cd98-4840c88bb770"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "len(docx_documents)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e606cf098a0c47388ab4a2f387b3beac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbdf731fc0294dd6a3c392012bc93896",
              "IPY_MODEL_63fc017f50be4f1f8c807f30a7ce11eb",
              "IPY_MODEL_4a47716561f74d8fa006b47f64a6837a"
            ],
            "layout": "IPY_MODEL_c69e38f11339431cac851d749a68ed9a"
          }
        },
        "bbdf731fc0294dd6a3c392012bc93896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8aab8494a2045bdbc09415b1418bd81",
            "placeholder": "​",
            "style": "IPY_MODEL_b4c4b57aa1f94e958c0d09c7558b7817",
            "value": "config.json: 100%"
          }
        },
        "63fc017f50be4f1f8c807f30a7ce11eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79141af480654819ae6c0981732355a2",
            "max": 1469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b217d584ee945a9bfedf1a6f33bfb67",
            "value": 1469
          }
        },
        "4a47716561f74d8fa006b47f64a6837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b2ee0c072e457db0d8989b8ed1aaac",
            "placeholder": "​",
            "style": "IPY_MODEL_b8bffaaa7c524cacb9cb2a5f67581e7f",
            "value": " 1.47k/1.47k [00:00&lt;00:00, 83.7kB/s]"
          }
        },
        "c69e38f11339431cac851d749a68ed9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8aab8494a2045bdbc09415b1418bd81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c4b57aa1f94e958c0d09c7558b7817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79141af480654819ae6c0981732355a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b217d584ee945a9bfedf1a6f33bfb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6b2ee0c072e457db0d8989b8ed1aaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8bffaaa7c524cacb9cb2a5f67581e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dee9511a788d41bab9d5fdd4b6e4f6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff2dfe08ffed46d2a633ac0b7b0eeec5",
              "IPY_MODEL_0f6d1f3099c2428bbce1f41dd04cdc4e",
              "IPY_MODEL_cc62a8b1ab8045caa46adac2bad7aef2"
            ],
            "layout": "IPY_MODEL_1bb6db9f42c047b8a65dcf4b93d84bbf"
          }
        },
        "ff2dfe08ffed46d2a633ac0b7b0eeec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7194f73d47a0456a8d462102d8b24288",
            "placeholder": "​",
            "style": "IPY_MODEL_822a272cf0724edb8e7fdccd5317e86e",
            "value": "model.safetensors: 100%"
          }
        },
        "0f6d1f3099c2428bbce1f41dd04cdc4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b3d3c90225407e943fc9dcb39ad53e",
            "max": 115434268,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb61550b3a774725a24e4e2a9ace15d5",
            "value": 115434268
          }
        },
        "cc62a8b1ab8045caa46adac2bad7aef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c7f0cf2749146668bd8df52ffe11fad",
            "placeholder": "​",
            "style": "IPY_MODEL_63a6b02ceb9340aba44b6891be465f0c",
            "value": " 115M/115M [00:00&lt;00:00, 195MB/s]"
          }
        },
        "1bb6db9f42c047b8a65dcf4b93d84bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7194f73d47a0456a8d462102d8b24288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822a272cf0724edb8e7fdccd5317e86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43b3d3c90225407e943fc9dcb39ad53e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb61550b3a774725a24e4e2a9ace15d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c7f0cf2749146668bd8df52ffe11fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a6b02ceb9340aba44b6891be465f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "677e724e773541898d7682c8b8d8ff61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f5f48c907f47f7a64bfdef3fb25c8b",
              "IPY_MODEL_3865ca39e1bc4a50ab761d215de760d7",
              "IPY_MODEL_432ad00a5bd84f1e9ed0cf8c881bbd74"
            ],
            "layout": "IPY_MODEL_1a85a1b9c4494ccfb66d5c351fdc42b3"
          }
        },
        "67f5f48c907f47f7a64bfdef3fb25c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b9cf97a2044db3a65ba7a013da8d80",
            "placeholder": "​",
            "style": "IPY_MODEL_c5aa729abde54a6692fec1aa39c4f49c",
            "value": "model.safetensors: 100%"
          }
        },
        "3865ca39e1bc4a50ab761d215de760d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c52584484948f1a8bd01beb4969315",
            "max": 46807446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46b6d128201f4f479c45b231ece02270",
            "value": 46807446
          }
        },
        "432ad00a5bd84f1e9ed0cf8c881bbd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34994b355d1841ec9019ec2a7c5dd408",
            "placeholder": "​",
            "style": "IPY_MODEL_68f35650c8ea40caa773596bf12c1845",
            "value": " 46.8M/46.8M [00:00&lt;00:00, 238MB/s]"
          }
        },
        "1a85a1b9c4494ccfb66d5c351fdc42b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b9cf97a2044db3a65ba7a013da8d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5aa729abde54a6692fec1aa39c4f49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73c52584484948f1a8bd01beb4969315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b6d128201f4f479c45b231ece02270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34994b355d1841ec9019ec2a7c5dd408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f35650c8ea40caa773596bf12c1845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}